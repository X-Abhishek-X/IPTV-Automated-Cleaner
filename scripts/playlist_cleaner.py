
import requests
import re
import concurrent.futures
from urllib.parse import urlparse
import time

# --- Configuration ---
SOURCE_URLS = [
    "https://iptv-org.github.io/iptv/countries/us.m3u",  # USA
    "https://iptv-org.github.io/iptv/countries/uk.m3u",  # UK
    "https://iptv-org.github.io/iptv/countries/in.m3u",  # India (Malayalam, Hindi, etc.)
    "https://iptv-org.github.io/iptv/categories/news.m3u", # Global News
    "https://iptv-org.github.io/iptv/categories/movies.m3u", # Global Movies
    "https://iptv-org.github.io/iptv/categories/sports.m3u" # Global Sports
]

OUTPUT_FILE = "clean_playlist.m3u"
MAX_WORKERS = 20  # Number of concurrent checks
TIMEOUT = 5       # Seconds to wait for a stream to respond

# --- Logic ---

def is_stream_working(url):
    """
    Checks if a stream URL is accessible and returns a valid status code.
    We use verify=False to ignore SSL errors which are common in IPTV.
    """
    try:
        with requests.get(url, stream=True, timeout=TIMEOUT, verify=False) as response:
            if response.status_code == 200:
                # Check Content-Type to ensure it's media (optional, sometimes misleading)
                # But at least make sure we got bytes.
                return True
    except:
        return False
    return False

def parse_m3u(content):
    """
    Parses a raw M3U string into a list of dicts: {'info': '#EXTINF...', 'url': 'http...'}
    """
    entries = []
    lines = content.splitlines()
    current_info = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
        
        if line.startswith("#EXTINF"):
            current_info = line
        elif line.startswith("#") and not line.startswith("#EXTINF"):
            # Other directives like #EXTVLCOPT
            pass
        else:
            # This must be the URL
            if current_info:
                entries.append({'info': current_info, 'url': line})
                current_info = None
            else:
                # URL without info? Skip or add dummy info
                pass
    return entries

def main():
    all_entries = []
    
    # 1. Download Sources
    print("üì• Downloading source playlists...")
    for source in SOURCE_URLS:
        try:
            print(f"   Fetching: {source}")
            resp = requests.get(source)
            if resp.status_code == 200:
                entries = parse_m3u(resp.text)
                print(f"   found {len(entries)} channels.")
                all_entries.extend(entries)
            else:
                print(f"   Failed (Status {resp.status_code})")
        except Exception as e:
            print(f"   Error: {e}")

    # Remove duplicates by URL
    unique_entries = {e['url']: e for e in all_entries}.values()
    print(f"\nüîç Total unique channels to check: {len(unique_entries)}")
    
    working_entries = []
    
    # 2. Validate Streams (Concurrent)
    print(f"üöÄ Validating streams (Concurrent checks: {MAX_WORKERS})...")
    
    # We use a ThreadPoolExecutor
    with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        # Submit all checks
        future_to_entry = {executor.submit(is_stream_working, entry['url']): entry for entry in unique_entries}
        
        checked_count = 0
        total = len(unique_entries)
        
        for future in concurrent.futures.as_completed(future_to_entry):
            entry = future_to_entry[future]
            checked_count += 1
            
            try:
                is_working = future.result()
                if is_working:
                    working_entries.append(entry)
                    print(f"‚úÖ [WORKING] {entry['url'][:50]}...") # Log working only to reduce noise
                else:
                     pass # print(f"‚ùå [DEAD] {entry['url'][:50]}...")
            except Exception as exc:
                pass
            
            if checked_count % 50 == 0:
                print(f"   Progress: {checked_count}/{total} checked ({len(working_entries)} working)...")

    # 3. Save Output
    print(f"\nüìù Writing clean playlist to {OUTPUT_FILE}...")
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("#EXTM3U\n")
        f.write("# Generated by Automated Playlist Cleaner\n")
        for entry in working_entries:
            f.write(f"{entry['info']}\n")
            f.write(f"{entry['url']}\n")
            
    print(f"üéâ Done! Saved {len(working_entries)} working channels.")

if __name__ == "__main__":
    # Disable SSL warnings for cleaner output
    import urllib3
    urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
    main()
